# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

# Experiment name
name: elasticvit_supernet
gpu: tesla_v100

# Name of output directory. Checkpoints and logs will be saved at `pwd`/output_dir
output_dir: /path_to_output_dir/supernet_training/
lib_dir: ./
offline_models_dir: offline_models/
training_device: gpu

# Dataset loader
dataloader:
  # Dataset to train/validate (choices: imagenet, cifar10)
  dataset: imagenet
  # Number of categories in the specified dataset (choices: 1000, 10)
  num_classes: 1000
  # Path to dataset directory
  path: /path_to_imagenet/imagenet/
  # Size of mini-batch (per GPU)
  batch_size: 96
  # Number of data loading workers
  workers: 16
  # Seeds random generators in a deterministic way (i.e., set all the seeds 0).
  # Please keep it true when resuming the experiment from a checkpoint
  deterministic: true
  # Load the model without DataParallel wrapping it
  serialized: false
  # Portion of training dataset to set aside for validation (range: [0, 1))
  val_split: 0.02
  rep_aug: false
  aug_type: auto
  aug:
    aa: rand-n1-m1-mstd0.5-inc1
    color_jitter: 0.4
    vflip: 0
    hflip: 0.5
    aug_repeats: 0
    aug_splits: 0
    train_interpolation: bicubic
    reprob: 0.01
    remode: pixel
    recount: 1
    mixup: True

resume:
  path: 
  lean: false

log:
  # Number of best scores to track and report
  num_best_scores: 3
  # Print frequency
  print_freq: 20

input_size: [3, 224, 224]
super_stem_channels: 128

#============================ Search Space ============================
sampling_mode: ['uniform', 'uniform', 'uniform', 'min']
head_type: mbv3
classifer_head_dim: [960, 1984]
head_dropout_prob: 0.2
norm_layer: LN # Norm layer for transformer. BN (BatchNorm) or LN (LayerNorm)
pre_norm: True # false for BN, true for LN
search_space:
  res: [128, 160, 176, 192, 224, 256, ]
  downsampling: [True, True, True, False, True, True]
  se: [False, True, True, False, False, False]
  classifier_mul: 6
  dw_downsampling: True
  stage: ['C', 'C', 'T', 'T', 'T', 'T']
  max_depth:     [5, 6, 5, 6, 6, 6]
  min_depth:     [2, 2, 1, 1, 1, 1]
  max_channels:  [32, 48, 96, 160, 288, 320]
  min_channels:  [16, 16, 48, 80, 144, 160]

  sample_prob: 1
  limit_flops: 500
  min_flops: 100 # memory bank B_a: [min_flops, limit_flops] step=100

  big_bank: True
  big_bank_choices: [700, 900, 1200] # memory bank B_b [700, 900, 1200]
  # if big_bank = True, then the final bank = B_a + B_b

  use_min_model: False
  swin: False # we do not support swin transformer 

  head_dims: 16
  flops_bound: 10
  bank_sampling_rate: 0.2
  sampling_rate_inc: 0.15

  flops_sampling_method: adjacent_step
  model_sampling_method: preference

  res_specific_rpn: False
  
  conv_ratio:    [6, 5, 4, 3]
  kr_size:       [3, 5] # unused now, just for notation

  mlp_ratio:     [5, 4, 3, 2]
  
  qk_scale:      [[1], [1], [1], [1]] # 4 sub lists for 4 transformer stages, the same below
  v_scale:       [[4, 3, 2], [4, 3, 2], [4, 3, 2], [4, 3, 2]]
  windows_size:  [[1], [1], [1], [1]]
  num_heads:     [[4], [6], [8], [10]] # override by the head_dims

  min_archs:
    200: [128, [16, 16, 16, 48, 80, 144, 160], [1, 2, 2, 1, 1, 1, 1], [1, 3, 3, 3, 3], [3, 3, 3, 3, 3], [2, 2, 2, 2], [3, 5, 9, 10], [1, 1, 1, 1], [1, 1, 1, 1], [2, 2, 2, 2]]
    400: [176, [16, 24, 32, 48, 96, 176, 224], [1, 3, 3, 2, 2, 2, 2], [1, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3], [2, 2, 3, 3, 2, 2, 2, 2], [3, 3, 6, 6, 11, 11, 14, 14], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2]]
    800: [224, [16, 24, 32, 64, 96, 192, 256], [1, 3, 3, 2, 2, 2, 2], [1, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3], [2, 2, 2, 2, 2, 2, 2, 2], [4, 4, 6, 6, 12, 12, 16, 16], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2]]

#============================ Training / Evaluation ============================
epochs: 600
smoothing: 0.1
clip_grad_value: 1.0
val_cycle: 50

alpha_divergence: true
distillation: true
teacher_model: swsl_resnext101_32x4d  
teacher_model_2: swsl_resnext101_32x8d
inplace_distillation: true

# Optimizer
opt: adamw
lr: 0.0005
momentum: 0.9
weight_decay: 0.05

# Learning rate scheduler
sched: cosine
min_lr: 0.000005
decay_rate: 0.1
warmup_epochs: 5
warmup_lr: 0.000001
decay_epochs: 30
cooldown_epochs: 10